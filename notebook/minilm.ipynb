{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penjelasan sedikit\n",
    "MiniLM itu pake teknik knowledge distilation buat ngecilin model transformer gede tapi ga kureng performnya. Adapun yg dia lakukan itu dengan skema:\n",
    "- Deep self-att distilation: Jadi kalo bert distil si layernya, si MiniLM dia \"belajar\" self att dari layer terakhirnya aja\n",
    "- Self att value-relation transfer: Selain distribusi si attentionnya (dot prod query.key) dia juga mmprhitungkan hubungan antar values dalam self-att\n",
    "- Teacher assistant: ini fixing transfer knowledge dari model teacher yg gede (model perantara ukuran sedang) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\text L_{kd} = \\sum_{x\\in D} \\text L(f_S(x),f_T(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "itu loss distilasinya, si $ \\text f_S(x) \\ $  adalah represent student model  $ \\text f_T(x) $ adalah teachernya. Terus L-nya lossnya sih bisa kek MSe atau KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nah kek yg biasa kita tau kan kalo di transformer itu kita rumusin dg <br>\n",
    "$$\n",
    "    \\text Attention(Q,K,V) = \\text softmax(\\frac {Q.K^T} {\\sqrt {d_k}}) V\n",
    "$$\n",
    "\n",
    "si MiniLM itu disini ngeoptimize dengan cara niru self-attnya dengan dua teknik:\n",
    "1. Self att distribution transfer <br>\n",
    "Jadi dia niru pake KL-Divergence\n",
    "$$\n",
    "    \\text L_{AT} = \\frac {1}{A_h|x|} \\sum_{a=1}^{A_h} \\sum_{t=1}^{|x|} D_{KL} (A^T_{L,a,t} || A^S_{M,a,t})\n",
    "$$\n",
    "2. Self att value relation transfer <br>\n",
    "Jadi slain distribusi att, MiniLm tu juga niru hubungan antar values mnggunakan dot product sm values\n",
    "$$\n",
    "    \\text V RT_{L} = \\text sofmax (\\frac{V_L V_L^T} {\\sqrt{d_k}})\n",
    "$$\n",
    "Lossnya habis itu diitung dengan KL-div sm kek yg si distribution transfer\n",
    "$$\n",
    "    \\text L_{VR} = \\frac {1}{A_h|x|} \\sum_{a=1}^{A_h} \\sum_{t=1}^{|x|} D_{KL} (VRT_{L,a,t} || VRT_{M,a,t})\n",
    "$$\n",
    "Nah baru total loss distilasi keduanya digabungin deh jadi $ \\text L = L_{AT} + L_{VR} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ket:\n",
    "$$\n",
    "    \\text A_h : attention headnya,\n",
    "    \\text |x| = panjang tokennya,\n",
    "    \\text a =  head,\n",
    "    \\text t = token\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perumusannya dapat ditulis dalam code sbg berikut bro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini yg distribution transfernya\n",
    "class SelfAttDistillation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,teacher_attn,student_attn):\n",
    "        loss = F.kl_div(\n",
    "                        F.log_softmax(student_attn,dim=-1),\n",
    "                        F.softmax(teacher_attn,dim=-1),\n",
    "                        reduction = 'batchmean'\n",
    "                        )\n",
    "        return loss\n",
    "    \n",
    "# ini yg self att value relation transfer\n",
    "class ValueRelationDistillation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, teacher_val, student_val):\n",
    "        teacher_relation = F.softmax(torch.bmm(teacher_val, teacher_val.transpose(1,2)) /\n",
    "                                     math.sqrt(teacher_val.size(-1)),dim=-1)\n",
    "        student_relation = F.softmax(torch.bmm(student_val, student_val.transpose(1,2)) /\n",
    "                                     math.sqrt(student_val.size(-1)),dim=-1)\n",
    "        loss = F.kl_div(\n",
    "            torch.log(student_relation), teacher_relation, reduction='batchmean'\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "# ini buat nyari total lossnya\n",
    "class MiniLMDistillation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn_distillation = SelfAttDistillation()\n",
    "        self.value_distillation = ValueRelationDistillation()\n",
    "\n",
    "    def forward(self, teacher_attn, student_attn, teacher_val, student_val):\n",
    "        loss_attn = self.attn_distillation(teacher_attn, student_attn)\n",
    "        loss_value_relation = self.value_distillation(teacher_attn, student_attn)\n",
    "\n",
    "        return loss_attn+loss_value_relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
